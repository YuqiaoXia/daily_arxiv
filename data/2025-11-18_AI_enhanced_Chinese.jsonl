{"id": "2511.10706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10706", "abs": "https://arxiv.org/abs/2511.10706", "authors": ["Zitong Zhang", "Hao Sun"], "title": "Differentiable Sparse Identification of Lagrangian Dynamics", "comment": null, "summary": "Data-driven discovery of governing equations from data remains a fundamental challenge in nonlinear dynamics. Although sparse regression techniques have advanced system identification, they struggle with rational functions and noise sensitivity in complex mechanical systems. The Lagrangian formalism offers a promising alternative, as it typically avoids rational expressions and provides a more concise representation of system dynamics. However, existing Lagrangian identification methods are significantly affected by measurement noise and limited data availability. This paper presents a novel differentiable sparse identification framework that addresses these limitations through three key contributions: (1) the first integration of cubic B-Spline approximation into Lagrangian system identification, enabling accurate representation of complex nonlinearities, (2) a robust equation discovery mechanism that effectively utilizes measurements while incorporating known physical constraints, (3) a recursive derivative computation scheme based on B-spline basis functions, effectively constraining higher-order derivatives and reducing noise sensitivity on second-order dynamical systems. The proposed method demonstrates superior performance and enables more accurate and reliable extraction of physical laws from noisy data, particularly in complex mechanical systems compared to baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u5fae\u5206\u7a00\u758f\u8bc6\u522b\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u53d1\u73b0\u590d\u6742\u673a\u68b0\u7cfb\u7edf\u7684\u62c9\u683c\u6717\u65e5\u52a8\u529b\u5b66\u65b9\u7a0b\uff0c\u901a\u8fc7B\u6837\u6761\u8fd1\u4f3c\u3001\u7269\u7406\u7ea6\u675f\u96c6\u6210\u548c\u9012\u5f52\u5bfc\u6570\u8ba1\u7b97\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u56de\u5f52\u6280\u672f\u5728\u590d\u6742\u673a\u68b0\u7cfb\u7edf\u4e2d\u5bf9\u6709\u7406\u51fd\u6570\u548c\u566a\u58f0\u654f\u611f\uff0c\u800c\u62c9\u683c\u6717\u65e5\u5f62\u5f0f\u4e3b\u4e49\u867d\u7136\u63d0\u4f9b\u4e86\u66f4\u7b80\u6d01\u7684\u7cfb\u7edf\u52a8\u529b\u5b66\u8868\u793a\uff0c\u4f46\u73b0\u6709\u8bc6\u522b\u65b9\u6cd5\u53d7\u6d4b\u91cf\u566a\u58f0\u548c\u6709\u9650\u6570\u636e\u53ef\u7528\u6027\u7684\u663e\u8457\u5f71\u54cd\u3002", "method": "\u96c6\u6210\u4e09\u6b21B\u6837\u6761\u8fd1\u4f3c\u5230\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\u8bc6\u522b\u4e2d\uff0c\u5f00\u53d1\u9c81\u68d2\u7684\u65b9\u7a0b\u53d1\u73b0\u673a\u5236\u7ed3\u5408\u5df2\u77e5\u7269\u7406\u7ea6\u675f\uff0c\u4ee5\u53ca\u57fa\u4e8eB\u6837\u6761\u57fa\u51fd\u6570\u7684\u9012\u5f52\u5bfc\u6570\u8ba1\u7b97\u65b9\u6848\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u673a\u68b0\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u53ef\u9760\u5730\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u63d0\u53d6\u7269\u7406\u5b9a\u5f8b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53ef\u5fae\u5206\u7a00\u758f\u8bc6\u522b\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\u8bc6\u522b\u4e2d\u7684\u566a\u58f0\u654f\u611f\u6027\u548c\u6570\u636e\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7684\u6570\u636e\u9a71\u52a8\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.10707", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10707", "abs": "https://arxiv.org/abs/2511.10707", "authors": ["Sirui Liang", "Pengfei Cao", "Jian Zhao", "Cong Huang", "Jun Zhao", "Kang Liu"], "title": "Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning", "comment": "accepted by aaai2026", "summary": "Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BREP ReFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u524d\u7f00\u751f\u6210\u3001\u5e72\u9884\u65e9\u671f\u63a8\u7406\u9636\u6bb5\u548c\u7ea6\u675f\u5e72\u9884\u5411\u91cf\u5e45\u5ea6\uff0c\u89e3\u51b3\u4e86ReFT\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "ReFT\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u96be\u4ee5\u751f\u6210\u6709\u6548\u7684\u63a8\u7406\u524d\u7f00\u4ee5\u53ca\u5e72\u6270\u6570\u503c\u7f16\u7801\u5bfc\u81f4\u9519\u8bef\u79ef\u7d2f\u3002", "method": "\u63d0\u51faBREP ReFT\u65b9\u6cd5\uff1a1\uff09\u622a\u65ad\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u521d\u59cb\u63a8\u7406\u524d\u7f00\u751f\u6210\uff1b2\uff09\u5e72\u9884\u65e9\u671f\u63a8\u7406\u9636\u6bb5\u9632\u6b62\u9519\u8bef\u79ef\u7d2f\uff1b3\uff09\u7ea6\u675f\u5e72\u9884\u5411\u91cf\u5e45\u5ea6\u907f\u514d\u5e72\u6270\u6570\u503c\u7f16\u7801\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cBREP\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6807\u51c6ReFT\u548c\u57fa\u4e8e\u6743\u91cd\u7684PEFT\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u6548\u679c\u3001\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BREP ReFT\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u4f18\u5316\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86ReFT\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11552", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11552", "abs": "https://arxiv.org/abs/2511.11552", "authors": ["Dawei Zhu", "Rui Meng", "Jiefeng Chen", "Sujian Li", "Tomas Pfister", "Jinsung Yoon"], "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding", "comment": null, "summary": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.", "AI": {"tldr": "DocLens\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\"\u653e\u5927\"\u8bc1\u636e\u6765\u89e3\u51b3\u957f\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4e2d\u7684\u8bc1\u636e\u5b9a\u4f4d\u95ee\u9898\uff0c\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4e2d\u5b58\u5728\u8bc1\u636e\u5b9a\u4f4d\u7684\u6839\u672c\u6311\u6218\uff1a\u96be\u4ee5\u68c0\u7d22\u76f8\u5173\u9875\u9762\u4e14\u5ffd\u7565\u89c6\u89c9\u5143\u7d20\u4e2d\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5bfc\u81f4\u6027\u80fd\u6709\u9650\u548c\u6a21\u578b\u5e7b\u89c9\u3002", "method": "\u63d0\u51faDocLens\u6846\u67b6\uff0c\u9996\u5148\u4ece\u5b8c\u6574\u6587\u6863\u5bfc\u822a\u5230\u76f8\u5173\u9875\u9762\u4e0a\u7684\u7279\u5b9a\u89c6\u89c9\u5143\u7d20\uff0c\u7136\u540e\u91c7\u7528\u91c7\u6837-\u88c1\u51b3\u673a\u5236\u751f\u6210\u5355\u4e00\u53ef\u9760\u7b54\u6848\u3002", "result": "\u4e0eGemini-2.5-Pro\u914d\u5bf9\uff0cDocLens\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "DocLens\u901a\u8fc7\u589e\u5f3a\u7684\u5b9a\u4f4d\u80fd\u529b\u5c55\u793a\u4e86\u5176\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u4e0a\uff0c\u8bc1\u660e\u4e86\u5176\u5b9a\u4f4d\u80fd\u529b\u7684\u5f3a\u5927\u6548\u679c\u3002"}}
