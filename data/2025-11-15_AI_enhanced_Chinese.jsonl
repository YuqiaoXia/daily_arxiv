{"id": "2511.10619", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10619", "abs": "https://arxiv.org/abs/2511.10619", "authors": ["Avrim Blum", "Marten Garicano", "Kavya Ravichandran", "Dravyansh Sharma"], "title": "Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem", "comment": "25 pages", "summary": "The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $\u03a9(k)$ and $\u03a9(\\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u53c2\u6570\u5316\u7684\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\u65cf\uff0c\u901a\u8fc7\u79bb\u7ebf\u6570\u636e\u5b66\u4e60\u8fd1\u6700\u4f18\u7b97\u6cd5\uff0c\u5728\u5956\u52b1\u66f2\u7ebf\u5177\u6709\u66f4\u5f3a\u51f9\u6027\u65f6\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u540c\u65f6\u4fdd\u8bc1\u5728\u826f\u597d\u5b9e\u4f8b\u4e0a\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u548c\u5728\u4e0d\u826f\u5b9e\u4f8b\u4e0a\u7684\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u3002", "motivation": "\u6539\u8fdb\u7684\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u7528\u4e8e\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5206\u914d\u52aa\u529b\uff0c\u5982\u6295\u8d44\u7814\u7a76\u3001\u4e34\u5e8a\u8bd5\u9a8c\u548c\u8d85\u53c2\u6570\u9009\u62e9\u3002\u73b0\u6709\u7b97\u6cd5\u7684\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u8f83\u4e3a\u60b2\u89c2\uff0c\u5b58\u5728\u03a9(k)\u548c\u03a9(\u221ak)\u7684\u4e0b\u754c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53c2\u6570\u5316\u7b97\u6cd5\u65cf\u548c\u6570\u636e\u4f9d\u8d56\u5206\u6790\u83b7\u5f97\u66f4\u5f3a\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u53c2\u6570\u5316\u7b97\u6cd5\u65cf\uff1a\u7b2c\u4e00\u4e2a\u5305\u542b\u5148\u524d\u5de5\u4f5c\u4e2d\u7684\u6700\u4f18\u968f\u673a\u7b97\u6cd5\uff0c\u5728\u5956\u52b1\u66f2\u7ebf\u5177\u6709\u66f4\u5f3a\u51f9\u6027\u65f6\u83b7\u5f97\u6700\u4f18k\u4f9d\u8d56\uff1b\u7b2c\u4e8c\u4e2a\u4fdd\u8bc1\u5728\u826f\u597d\u5b9e\u4f8b\u4e0a\u8bc6\u522b\u6700\u4f73\u81c2\uff0c\u5728\u4e0d\u826f\u5b9e\u4f8b\u4e0a\u56de\u9000\u5230\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u3002\u901a\u8fc7\u79bb\u7ebf\u6570\u636e\u5b66\u4e60\u8fd1\u6700\u4f18\u7b97\u6cd5\u3002", "result": "\u5728\u5956\u52b1\u66f2\u7ebf\u6ee1\u8db3\u989d\u5916\u51f9\u6027\u7279\u6027\u65f6\uff0c\u7b2c\u4e00\u4e2a\u7b97\u6cd5\u65cf\u53ef\u4ee5\u5b9e\u73b0\u66f4\u5f3a\u7684\u4fdd\u8bc1\uff0c\u5177\u6709\u6700\u4f18\u7684k\u4f9d\u8d56\u3002\u7b2c\u4e8c\u4e2a\u7b97\u6cd5\u65cf\u5b9e\u73b0\u4e86\u6570\u636e\u4f9d\u8d56\u7684\u4fdd\u8bc1\uff0c\u65e0\u9700\u9a8c\u8bc1\u5047\u8bbe\u662f\u5426\u6ee1\u8db3\u3002", "conclusion": "\u901a\u8fc7\u53c2\u6570\u5316\u7b97\u6cd5\u65cf\u548c\u7edf\u8ba1\u5b66\u4e60\u89c6\u89d2\uff0c\u672c\u6587\u5728\u6539\u8fdb\u7684\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u6570\u636e\u4f9d\u8d56\u6027\u80fd\u4fdd\u8bc1\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u6700\u574f\u60c5\u51b5\u5206\u6790\u7684\u9650\u5236\u3002"}}
{"id": "2511.10615", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10615", "abs": "https://arxiv.org/abs/2511.10615", "authors": ["Shruti Singh Baghel", "Yash Pratap Singh Rathore", "Sushovan Jena", "Anurag Pradhan", "Amit Shukla", "Arnav Bhavsar", "Pawan Goyal"], "title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals", "comment": "8 pages", "summary": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\uff08500M\u548c2.2B\uff09\u7684SmolVLM2\u6a21\u578b\u5728\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e13\u95e8\u9488\u5bf9BLV\u7528\u6237\u7684\u4e24\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u548c\u79fb\u52a8\u8bbe\u5907\u90e8\u7f72\u65b9\u6848\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9ad8\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u90e8\u7f72\u9700\u6c42\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4f9d\u8d56\u8be6\u7ec6\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u63cf\u8ff0\u7684\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u3002", "method": "\u4f7f\u7528500M\u548c2.2B\u53c2\u6570\u7684SmolVLM2\u53d8\u4f53\u5728AVCaps\uff08\u6237\u5916\uff09\u548cCharades\uff08\u5ba4\u5185\uff09\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff1b\u5f15\u5165\u4e24\u4e2a\u4e13\u95e8\u9488\u5bf9BLV\u53ef\u8bbf\u95ee\u6027\u8bc4\u4f30\u7684\u65b0\u6846\u67b6\uff1a\u591a\u4e0a\u4e0b\u6587BLV\u6846\u67b6\u548c\u5bfc\u822a\u8f85\u52a9\u6846\u67b6\uff1b\u7cfb\u7edf\u8bc4\u4f30\u56db\u79cd\u4e0d\u540c\u63d0\u793a\u8bbe\u8ba1\u7b56\u7565\uff1b\u5728\u667a\u80fd\u624b\u673a\u4e0a\u90e8\u7f72\u6a21\u578b\uff0c\u8bc4\u4f30FP32\u548cINT8\u7cbe\u5ea6\u53d8\u4f53\u3002", "result": "\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u5bf9\u6bd4\uff0c\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5b9e\u9645\u6027\u80fd\u7ea6\u675f\u3002", "conclusion": "\u7814\u7a76\u6a21\u578b\u89c4\u6a21\u5bf9\u53ef\u8bbf\u95ee\u6027\u63cf\u8ff0\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u4e3aBLV\u7528\u6237\u63d0\u4f9b\u66f4\u5b9e\u7528\u7684\u89c6\u9891\u63cf\u8ff0\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u53ef\u884c\u6027\u3002"}}
{"id": "2511.10648", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10648", "abs": "https://arxiv.org/abs/2511.10648", "authors": ["Jiahao Wang", "Weiye Xu", "Aijun Yang", "Wengang Zhou", "Lewei Lu", "Houqiang Li", "Xiaohua Wang", "Jinguo Zhu"], "title": "Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling", "comment": "Accepted to NeurIPS 2025 (The Thirty-Ninth Annual Conference on Neural Information Processing Systems)", "summary": "Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u81ea\u6211\u4e00\u81f4\u6027\u91c7\u6837\uff08SCS\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u6b63\u786e\u731c\u6d4b\u4f46\u9519\u8bef\u63a8\u7406\u8f68\u8ff9\u83b7\u5f97\u76f8\u540c\u5956\u52b1\u7684\u95ee\u9898\u3002\u901a\u8fc7\u89c6\u89c9\u6270\u52a8\u548c\u8f68\u8ff9\u91cd\u91c7\u6837\u83b7\u5f97\u4e00\u81f4\u6027\u8bc4\u5206\uff0c\u5728\u7b56\u7565\u66f4\u65b0\u65f6\u964d\u4f4e\u4e0d\u53ef\u9760\u8f68\u8ff9\u7684\u6743\u91cd\u3002", "motivation": "\u5728\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u9009\u9898\u8bbe\u7f6e\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u969c\u788d\uff1a\u901a\u8fc7\u9519\u8bef\u63a8\u7406\u94fe\u4f46\u6b63\u786e\u731c\u6d4b\u9009\u9879\u7684\u8f68\u8ff9\u4e0e\u771f\u6b63\u63a8\u7406\u7684\u8f68\u8ff9\u83b7\u5f97\u76f8\u540c\u5956\u52b1\uff0c\u8fd9\u5f71\u54cd\u4e86\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51faSCS\u65b9\u6cd5\uff1a\u5bf9\u6bcf\u4e2a\u95ee\u9898\u5f15\u5165\u5c0f\u89c6\u89c9\u6270\u52a8\uff0c\u5bf9\u521d\u59cb\u8f68\u8ff9\u8fdb\u884c\u91cd\u590d\u622a\u65ad\u548c\u91cd\u91c7\u6837\uff0c\u901a\u8fc7\u7ed3\u679c\u8f68\u8ff9\u7684\u4e00\u81f4\u6027\u4ea7\u751f\u53ef\u5fae\u5206\u7684\u4e00\u81f4\u6027\u8bc4\u5206\uff0c\u5728\u7b56\u7565\u66f4\u65b0\u65f6\u964d\u4f4e\u4e0d\u53ef\u9760\u8f68\u8ff9\u7684\u6743\u91cd\u3002", "result": "\u5728Qwen2.5-VL-7B-Instruct\u6a21\u578b\u4e0a\uff0c\u5c06SCS\u96c6\u6210\u5230RLOO\u3001GRPO\u548cREINFORCE++\u7cfb\u5217\u4e2d\uff0c\u5728\u516d\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe7.7\u4e2a\u767e\u5206\u70b9\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u3002\u5728Qwen2.5-VL-3B-Instruct\u548cInternVL3-8B\u6a21\u578b\u4e0a\u4e5f\u83b7\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SCS\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7ed3\u679c\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6b63\u786e\u731c\u6d4b\u4f46\u9519\u8bef\u63a8\u7406\u8f68\u8ff9\u7684\u95ee\u9898\u3002"}}
