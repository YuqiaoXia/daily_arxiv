<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本文评估了不同参数规模（500M和2.2B）的SmolVLM2模型在盲人和低视力用户视频描述任务中的表现，提出了专门针对BLV用户的两个评估框架，并测试了不同提示策略和移动设备部署方案。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在视频描述任务中表现出色，但其高内存、计算和部署需求限制了实际应用，特别是对于依赖详细、上下文感知描述的盲人和低视力用户。

Method: 使用500M和2.2B参数的SmolVLM2变体在AVCaps（户外）和Charades（室内）数据集上进行评估；引入两个专门针对BLV可访问性评估的新框架：多上下文BLV框架和导航辅助框架；系统评估四种不同提示设计策略；在智能手机上部署模型，评估FP32和INT8精度变体。

Result: 通过专门设计的评估框架和不同参数规模的模型对比，评估了模型在资源受限移动设备上的实际性能约束。

Conclusion: 研究模型规模对可访问性描述质量的影响，为BLV用户提供更实用的视频描述解决方案，特别是在移动设备上的部署可行性。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 本文提出自我一致性采样（SCS）方法，解决多模态大语言模型在强化学习中正确猜测但错误推理轨迹获得相同奖励的问题。通过视觉扰动和轨迹重采样获得一致性评分，在策略更新时降低不可靠轨迹的权重。


<details>
  <summary>Details</summary>
Motivation: 在多模态推理基准测试的多选题设置中，强化学习面临一个被忽视的障碍：通过错误推理链但正确猜测选项的轨迹与真正推理的轨迹获得相同奖励，这影响了学习效果。

Method: 提出SCS方法：对每个问题引入小视觉扰动，对初始轨迹进行重复截断和重采样，通过结果轨迹的一致性产生可微分的一致性评分，在策略更新时降低不可靠轨迹的权重。

Result: 在Qwen2.5-VL-7B-Instruct模型上，将SCS集成到RLOO、GRPO和REINFORCE++系列中，在六个多模态基准测试上准确率提升高达7.7个百分点，计算开销可忽略。在Qwen2.5-VL-3B-Instruct和InternVL3-8B模型上也获得显著提升。

Conclusion: SCS为多模态大语言模型中的结果奖励强化学习提供了一个简单通用的解决方案，有效解决了正确猜测但错误推理轨迹的问题。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem](https://arxiv.org/abs/2511.10619)
*Avrim Blum,Marten Garicano,Kavya Ravichandran,Dravyansh Sharma*

Main category: cs.LG

TL;DR: 本文提出了两个参数化的多臂老虎机算法族，通过离线数据学习近最优算法，在奖励曲线具有更强凹性时获得更好的性能保证，同时保证在良好实例上的最佳臂识别和在不良实例上的最坏情况保证。


<details>
  <summary>Details</summary>
Motivation: 改进的多臂老虎机问题用于在不确定性下分配努力，如投资研究、临床试验和超参数选择。现有算法的最坏情况保证较为悲观，存在Ω(k)和Ω(√k)的下界。本文旨在通过参数化算法族和数据依赖分析获得更强的性能保证。

Method: 提出了两个参数化算法族：第一个包含先前工作中的最优随机算法，在奖励曲线具有更强凹性时获得最优k依赖；第二个保证在良好实例上识别最佳臂，在不良实例上回退到最坏情况保证。通过离线数据学习近最优算法。

Result: 在奖励曲线满足额外凹性特性时，第一个算法族可以实现更强的保证，具有最优的k依赖。第二个算法族实现了数据依赖的保证，无需验证假设是否满足。

Conclusion: 通过参数化算法族和统计学习视角，本文在改进的多臂老虎机问题中实现了更强的数据依赖性能保证，突破了传统最坏情况分析的限制。

Abstract: The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.

</details>
